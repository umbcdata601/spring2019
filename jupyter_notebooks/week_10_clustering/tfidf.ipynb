{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://en.wikipedia.org/wiki/Tf%E2%80%93idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import fnmatch # https://docs.python.org/3/library/fnmatch.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source https://stevenloria.com/tf-idf/ <BR>\n",
    "\n",
    "Caveat: this post now uses TextBlob for breaking up the text into words and getting the word counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def term_freq(term, list_of_words_in_document):\n",
    "    \"\"\"\n",
    "    computes \"term frequency\" which is the number of times a word appears in a document, \n",
    "    normalized by dividing by the total number of words in document. \n",
    "    \"\"\"\n",
    "    return list_of_words_in_document.count(term)/(len(list_of_words_in_document)*1.0)\n",
    "\n",
    "def number_of_documents_containing(term,all_documents):\n",
    "    \"\"\"\n",
    "    Returns the number of documents containing word. \n",
    "    \"\"\"\n",
    "    countr=0\n",
    "    for this_doc in all_documents:\n",
    "        if (term in this_doc):\n",
    "            countr+=1\n",
    "    return countr\n",
    "\n",
    "def inverse_doc_freq(term, all_documents):\n",
    "    \"\"\"\n",
    "    computes \"inverse document frequency\" which measures \n",
    "    how common a word is among all documents in corpus. \n",
    "    The more common a word is, the lower its idf. \n",
    "    Take the ratio of the total number of documents to the number of documents containing word, \n",
    "    then take the log of that. Add 1 to the divisor to prevent division by zero.\n",
    "    \"\"\"\n",
    "    return math.log(len(all_documents) / ( 1.0 + number_of_documents_containing(term, all_documents)))\n",
    "\n",
    "def tfidf(term, list_of_words_in_document, all_documents):\n",
    "    \"\"\"\n",
    "    computes the TF-IDF score. It's the product of tf and idf.\n",
    "    \"\"\"\n",
    "    return term_freq(term, list_of_words_in_document) * inverse_doc_freq(term, all_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \\*.dat files in the directory have only key words from each file\n",
    "\n",
    "Convert the .dat contents to lists per document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_documents={}\n",
    "all_words_from_all_docs=[]\n",
    "all_terms=[]\n",
    "foldr='../week_03_getting_data/essays/'\n",
    "fname='*.dat'\n",
    "for file_name in os.listdir(foldr):\n",
    "    #print(file_name)\n",
    "    if fnmatch.fnmatch(file_name, fname): # Unix shell-style wildcards\n",
    "        print(file_name)\n",
    "        with open(foldr+file_name,'r') as fil:\n",
    "            words_in_file=fil.read().split(\"\\n\")\n",
    "        # remove empty strings from list of words\n",
    "        while \"\" in words_in_file:\n",
    "            words_in_file.remove(\"\")\n",
    "        # save the words per file as value in a dictionary\n",
    "        all_documents[file_name]=words_in_file\n",
    "        print('has',len(words_in_file),'words\\n')\n",
    "        # also save all the words to a list\n",
    "        for this_word in words_in_file:\n",
    "            all_words_from_all_docs.append(this_word)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words_from_all_docs = list(set(all_words_from_all_docs))\n",
    "len(all_words_from_all_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample sizes are small, so results are not reliable representations of the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc_name, word_list_in_this_doc in all_documents.items():\n",
    "    if (len(word_list_in_this_doc)==0):\n",
    "        print(\"error: empty input file\"+doc_name)\n",
    "    else:\n",
    "        dic_of_terms={}\n",
    "        for this_term in word_list_in_this_doc:\n",
    "            dic_of_terms[this_term] = tfidf(this_term, word_list_in_this_doc, all_words_from_all_docs)\n",
    "        #print(dic_of_terms)\n",
    "        print('\\n'+doc_name)\n",
    "        terms_in_doc_sorted_by_score=sorted(dic_of_terms.items(), key=lambda x: x[1], reverse=True)\n",
    "        # first 40 words by importance\n",
    "        for this_tup in terms_in_doc_sorted_by_score[0:40]:\n",
    "            print(this_tup)\n",
    "#        for indx in range(10):\n",
    "#            print(terms_in_doc_sorted_by_score[indx][0] + \":\"+str(terms_in_doc_sorted_by_score[indx][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
